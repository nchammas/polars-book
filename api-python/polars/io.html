<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js ayu">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>io - Polars - Python Reference Guide</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="../favicon.svg">
        
        
        <link rel="shortcut icon" href="../favicon.png">
        
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        
        <link rel="stylesheet" href="../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="../theme/css/style.css">
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "ayu";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('ayu')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="../polars.html">polars</a></li><li class="chapter-item "><a href="../polars/cfg.html">cfg</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/cfg/Config.html">Config</a></li></ol></li><li class="chapter-item "><a href="../polars/convert.html">convert</a></li><li class="chapter-item "><a href="../polars/datatypes.html">datatypes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/datatypes/DataType.html">DataType</a></li><li class="chapter-item "><a href="../polars/datatypes/Int8.html">Int8</a></li><li class="chapter-item "><a href="../polars/datatypes/Int16.html">Int16</a></li><li class="chapter-item "><a href="../polars/datatypes/Int32.html">Int32</a></li><li class="chapter-item "><a href="../polars/datatypes/Int64.html">Int64</a></li><li class="chapter-item "><a href="../polars/datatypes/UInt8.html">UInt8</a></li><li class="chapter-item "><a href="../polars/datatypes/UInt16.html">UInt16</a></li><li class="chapter-item "><a href="../polars/datatypes/UInt32.html">UInt32</a></li><li class="chapter-item "><a href="../polars/datatypes/UInt64.html">UInt64</a></li><li class="chapter-item "><a href="../polars/datatypes/Float32.html">Float32</a></li><li class="chapter-item "><a href="../polars/datatypes/Float64.html">Float64</a></li><li class="chapter-item "><a href="../polars/datatypes/Boolean.html">Boolean</a></li><li class="chapter-item "><a href="../polars/datatypes/Utf8.html">Utf8</a></li><li class="chapter-item "><a href="../polars/datatypes/List.html">List</a></li><li class="chapter-item "><a href="../polars/datatypes/Date.html">Date</a></li><li class="chapter-item "><a href="../polars/datatypes/Datetime.html">Datetime</a></li><li class="chapter-item "><a href="../polars/datatypes/Duration.html">Duration</a></li><li class="chapter-item "><a href="../polars/datatypes/Time.html">Time</a></li><li class="chapter-item "><a href="../polars/datatypes/Object.html">Object</a></li><li class="chapter-item "><a href="../polars/datatypes/Categorical.html">Categorical</a></li></ol></li><li class="chapter-item "><a href="../polars/datatypes_constructor.html">datatypes_constructor</a></li><li class="chapter-item "><a href="../polars/internals.html">internals</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/internals/construction.html">construction</a></li><li class="chapter-item "><a href="../polars/internals/expr.html">expr</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/internals/expr/Expr.html">Expr</a></li><li class="chapter-item "><a href="../polars/internals/expr/ExprListNameSpace.html">ExprListNameSpace</a></li><li class="chapter-item "><a href="../polars/internals/expr/ExprStringNameSpace.html">ExprStringNameSpace</a></li><li class="chapter-item "><a href="../polars/internals/expr/ExprDateTimeNameSpace.html">ExprDateTimeNameSpace</a></li></ol></li><li class="chapter-item "><a href="../polars/internals/frame.html">frame</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/internals/frame/DataFrame.html">DataFrame</a></li><li class="chapter-item "><a href="../polars/internals/frame/DynamicGroupBy.html">DynamicGroupBy</a></li><li class="chapter-item "><a href="../polars/internals/frame/GroupBy.html">GroupBy</a></li><li class="chapter-item "><a href="../polars/internals/frame/PivotOps.html">PivotOps</a></li><li class="chapter-item "><a href="../polars/internals/frame/GBSelection.html">GBSelection</a></li></ol></li><li class="chapter-item "><a href="../polars/internals/functions.html">functions</a></li><li class="chapter-item "><a href="../polars/internals/lazy_frame.html">lazy_frame</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/internals/lazy_frame/LazyFrame.html">LazyFrame</a></li><li class="chapter-item "><a href="../polars/internals/lazy_frame/LazyGroupBy.html">LazyGroupBy</a></li></ol></li><li class="chapter-item "><a href="../polars/internals/lazy_functions.html">lazy_functions</a></li><li class="chapter-item "><a href="../polars/internals/series.html">series</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/internals/series/Series.html">Series</a></li><li class="chapter-item "><a href="../polars/internals/series/StringNameSpace.html">StringNameSpace</a></li><li class="chapter-item "><a href="../polars/internals/series/ListNameSpace.html">ListNameSpace</a></li><li class="chapter-item "><a href="../polars/internals/series/DateTimeNameSpace.html">DateTimeNameSpace</a></li><li class="chapter-item "><a href="../polars/internals/series/SeriesIter.html">SeriesIter</a></li></ol></li><li class="chapter-item "><a href="../polars/internals/whenthen.html">whenthen</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/internals/whenthen/WhenThenThen.html">WhenThenThen</a></li><li class="chapter-item "><a href="../polars/internals/whenthen/WhenThen.html">WhenThen</a></li><li class="chapter-item "><a href="../polars/internals/whenthen/When.html">When</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../polars/io.html" class="active">io</a></li><li class="chapter-item "><a href="../polars/string_cache.html">string_cache</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../polars/string_cache/StringCache.html">StringCache</a></li></ol></li><li class="chapter-item "><a href="../polars/testing.html">testing</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu (default)</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Polars - Python Reference Guide</h1>

                    <div class="right-buttons">
                        
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="module-polarsio"><a class="header" href="#module-polarsio">Module <code>polars.io</code></a></h1>
<p><strong>Functions</strong></p>
<ul>
<li><a href="#polarsioupdate_columns"><code>update_columns()</code></a>: </li>
<li><a href="#polarsioread_csv"><code>read_csv()</code></a>: Read a CSV file into a Dataframe.</li>
<li><a href="#polarsioscan_csv"><code>scan_csv()</code></a>: Lazily read from a CSV file or multiple files via glob patterns.</li>
<li><a href="#polarsioscan_ipc"><code>scan_ipc()</code></a>: Lazily read from an Arrow IPC (Feather v2) file or multiple files via glob patterns.</li>
<li><a href="#polarsioscan_parquet"><code>scan_parquet()</code></a>: Lazily read from a parquet file or multiple files via glob patterns.</li>
<li><a href="#polarsioread_ipc_schema"><code>read_ipc_schema()</code></a>: Get a schema of the IPC file without reading data.</li>
<li><a href="#polarsioread_ipc"><code>read_ipc()</code></a>: Read into a DataFrame from Arrow IPC (Feather v2) file.</li>
<li><a href="#polarsioread_parquet"><code>read_parquet()</code></a>: Read into a DataFrame from a parquet file.</li>
<li><a href="#polarsioread_json"><code>read_json()</code></a>: Read into a DataFrame from JSON format.</li>
<li><a href="#polarsioread_sql"><code>read_sql()</code></a>: Read a SQL query into a DataFrame.</li>
</ul>
<h2 id="functions"><a class="header" href="#functions">Functions</a></h2>
<p><div class='function-wrap'></raw></p>
<h3 id="polarsioupdate_columns"><a class="header" href="#polarsioupdate_columns"><code>polars.io.update_columns</code></a></h3>
<pre><code class="language-python">update_columns(
    df: DataFrame, 
    new_columns: List[str],
) -&gt; DataFrame:
</code></pre>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def update_columns(df: DataFrame, new_columns: List[str]) -&gt; DataFrame:
    if df.width &gt; len(new_columns):
        cols = df.columns
        for i, name in enumerate(new_columns):
            cols[i] = name
        new_columns = cols
    df.columns = new_columns
    return df
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioread_csv"><a class="header" href="#polarsioread_csv"><code>polars.io.read_csv</code></a></h3>
<pre><code class="language-python">read_csv(
    file: Union[str, TextIO, BytesIO, Path, BinaryIO, bytes], 
    has_header: bool, 
    columns: OptionalUnion[List[int], List[str]], 
    new_columns: OptionalList[str], 
    sep: str, 
    comment_char: Optionalstr, 
    quote_char: Optionalstr, 
    skip_rows: int, 
    dtypes: OptionalUnion[Mapping[str, TypeDataType], List[TypeDataType]], 
    null_values: OptionalUnion[str, List[str], Dict[str, str]], 
    ignore_errors: bool, 
    parse_dates: bool, 
    n_threads: Optionalint, 
    infer_schema_length: Optionalint, 
    batch_size: int, 
    n_rows: Optionalint, 
    encoding: str, 
    low_memory: bool, 
    rechunk: bool, 
    use_pyarrow: bool, 
    storage_options: OptionalDict, 
    offset_schema_inference: int, 
    **kwargs,
) -&gt; DataFrame:
</code></pre>
<p>Read a CSV file into a Dataframe.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>file</code>]: Path to a file or a file like object.
By file-like object, we refer to objects with a <code>read()</code>
method, such as a file handler (e.g. via builtin <code>open</code>
function) or <code>StringIO</code> or <code>BytesIO</code>.
If <code>fsspec</code> is installed, it will be used to open remote
files.</li>
<li>[<code>has_header</code>]: Indicate if the first row of dataset is a header or not.
If set to False, column names will be autogenrated in the
following format: <code>column_x</code>, with <code>x</code> being an
enumeration over every column in the dataset starting at 1.</li>
<li>[<code>columns</code>]: Columns to select. Accepts a list of column indices (starting
at zero) or a list of column names.</li>
<li>[<code>new_columns</code>]: Rename columns right after parsing the CSV file. If the given</li>
<li>[<code>    list is shorter than the width of the DataFrame the remaining</code>]: columns will have their original name.</li>
<li>[<code>sep</code>]: Character to use as delimiter in the file.</li>
<li>[<code>comment_char</code>]: Character that indicates the start of a comment line, for
instance <code>#</code>.</li>
<li>[<code>quote_char</code>]: Single byte character used for csv quoting, default = ''.
Set to None to turn off special handling and escaping of quotes.</li>
<li>[<code>skip_rows</code>]: Start reading after <code>skip_rows</code> lines.</li>
<li>[<code>dtypes</code>]: Overwrite dtypes during inference.</li>
<li>[<code>null_values</code>]: Values to interpret as null values. You can provide a:
- <code>str</code>: All values equal to this string will be null.
- <code>List[str]</code>: A null value per column.
- <code>Dict[str, str]</code>: A dictionary that maps column name to a
null value string.</li>
<li>[<code>ignore_errors</code>]: Try to keep reading lines if some lines yield errors.
First try <code>infer_schema_length=0</code> to read all columns as
<code>pl.Utf8</code> to check which values might cause an issue.</li>
<li>[<code>parse_dates</code>]: Try to automatically parse dates. If this does not succeed,
the column remains of data type <code>pl.Utf8</code>.</li>
<li>[<code>n_threads</code>]: Number of threads to use in csv parsing.
Defaults to the number of physical cpu's of your system.</li>
<li>[<code>infer_schema_length</code>]: Maximum number of lines to read to infer schema.
If set to 0, all columns will be read as <code>pl.Utf8</code>.
If set to <code>None</code>, a full table scan will be done (slow).</li>
<li>[<code>batch_size</code>]: Number of lines to read into the buffer at once.
Modify this to change performance.</li>
<li>[<code>n_rows</code>]: Stop reading from CSV file after reading <code>n_rows</code>.
During multi-threaded parsing, an upper bound of <code>n_rows</code>
rows cannot be guaranteed.</li>
<li>[<code>encoding</code>]: Allowed encodings: <code>utf8</code> or <code>utf8-lossy</code>.
Lossy means that invalid utf8 values are replaced with <code>�</code>
characters.</li>
<li>[<code>low_memory</code>]: Reduce memory usage at expense of performance.</li>
<li>[<code>rechunk</code>]: Make sure that all columns are contiguous in memory by
aggregating the chunks into a single array.</li>
<li>[<code>use_pyarrow</code>]: Try to use pyarrow's native CSV parser.
This is not always possible. The set of arguments given to
this function determines if it is possible to use pyarrow's
native parser. Note that pyarrow and polars may have a
different strategy regarding type inference.</li>
<li>[<code>storage_options</code>]: Extra options that make sense for <code>fsspec.open()</code> or a
particular storage connection.
e.g. host, port, username, password, etc.</li>
<li>[<code>offset_schema_inference</code>]: Start schema parsing of the header at this offset</li>
</ul>
<p><strong>Returns</strong></p>
<p>DataFrame</p>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def read_csv(
    file: Union[str, TextIO, BytesIO, Path, BinaryIO, bytes],
    has_header: bool = True,
    columns: Optional[Union[List[int], List[str]]] = None,
    new_columns: Optional[List[str]] = None,
    sep: str = &quot;,&quot;,
    comment_char: Optional[str] = None,
    quote_char: Optional[str] = r'&quot;',
    skip_rows: int = 0,
    dtypes: Optional[Union[Mapping[str, Type[DataType]], List[Type[DataType]]]] = None,
    null_values: Optional[Union[str, List[str], Dict[str, str]]] = None,
    ignore_errors: bool = False,
    parse_dates: bool = False,
    n_threads: Optional[int] = None,
    infer_schema_length: Optional[int] = 100,
    batch_size: int = 8192,
    n_rows: Optional[int] = None,
    encoding: str = &quot;utf8&quot;,
    low_memory: bool = False,
    rechunk: bool = True,
    use_pyarrow: bool = False,
    storage_options: Optional[Dict] = None,
    offset_schema_inference: int = 0,
    **kwargs: Any,
) -&gt; DataFrame:
    &quot;&quot;&quot;
    Read a CSV file into a Dataframe.

    Parameters
    ----------
    file
        Path to a file or a file like object.
        By file-like object, we refer to objects with a ``read()``
        method, such as a file handler (e.g. via builtin ``open``
        function) or ``StringIO`` or ``BytesIO``.
        If ``fsspec`` is installed, it will be used to open remote
        files.
    has_header
        Indicate if the first row of dataset is a header or not.
        If set to False, column names will be autogenrated in the
        following format: ``column_x``, with ``x`` being an
        enumeration over every column in the dataset starting at 1.
    columns
        Columns to select. Accepts a list of column indices (starting
        at zero) or a list of column names.
    new_columns
        Rename columns right after parsing the CSV file. If the given
        list is shorter than the width of the DataFrame the remaining
        columns will have their original name.
    sep
        Character to use as delimiter in the file.
    comment_char
        Character that indicates the start of a comment line, for
        instance ``#``.
    quote_char
        Single byte character used for csv quoting, default = ''.
        Set to None to turn off special handling and escaping of quotes.
    skip_rows
        Start reading after ``skip_rows`` lines.
    dtypes
        Overwrite dtypes during inference.
    null_values
        Values to interpret as null values. You can provide a:
          - ``str``: All values equal to this string will be null.
          - ``List[str]``: A null value per column.
          - ``Dict[str, str]``: A dictionary that maps column name to a
                                null value string.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
        First try ``infer_schema_length=0`` to read all columns as
        ``pl.Utf8`` to check which values might cause an issue.
    parse_dates
        Try to automatically parse dates. If this does not succeed,
        the column remains of data type ``pl.Utf8``.
    n_threads
        Number of threads to use in csv parsing.
        Defaults to the number of physical cpu's of your system.
    infer_schema_length
        Maximum number of lines to read to infer schema.
        If set to 0, all columns will be read as ``pl.Utf8``.
        If set to ``None``, a full table scan will be done (slow).
    batch_size
        Number of lines to read into the buffer at once.
        Modify this to change performance.
    n_rows
        Stop reading from CSV file after reading ``n_rows``.
        During multi-threaded parsing, an upper bound of ``n_rows``
        rows cannot be guaranteed.
    encoding
        Allowed encodings: ``utf8`` or ``utf8-lossy``.
        Lossy means that invalid utf8 values are replaced with ``�``
        characters.
    low_memory
        Reduce memory usage at expense of performance.
    rechunk
        Make sure that all columns are contiguous in memory by
        aggregating the chunks into a single array.
    use_pyarrow
        Try to use pyarrow's native CSV parser.
        This is not always possible. The set of arguments given to
        this function determines if it is possible to use pyarrow's
        native parser. Note that pyarrow and polars may have a
        different strategy regarding type inference.
    storage_options
        Extra options that make sense for ``fsspec.open()`` or a
        particular storage connection.
        e.g. host, port, username, password, etc.
    offset_schema_inference
        Start schema parsing of the header at this offset

    Returns
    -------
    DataFrame
    &quot;&quot;&quot;

    # Map legacy arguments to current ones and remove them from kwargs.
    has_header = kwargs.pop(&quot;has_headers&quot;, has_header)
    dtypes = kwargs.pop(&quot;dtype&quot;, dtypes)
    n_rows = kwargs.pop(&quot;stop_after_n_rows&quot;, n_rows)

    if columns is None:
        columns = kwargs.pop(&quot;projection&quot;, None)

    projection, columns = handle_projection_columns(columns)

    if isinstance(file, bytes) and len(file) == 0:
        raise ValueError(&quot;no date in bytes&quot;)

    storage_options = storage_options or {}

    if columns and not has_header:
        for column in columns:
            if isinstance(column, str) and not column.startswith(&quot;column_&quot;):
                raise ValueError(
                    'Specified column names do not start with &quot;column_&quot;, '
                    &quot;but autogenerated header names were requested.&quot;
                )

    if use_pyarrow and not _PYARROW_AVAILABLE:
        raise ImportError(
            &quot;'pyarrow' is required when using 'read_csv(..., use_pyarrow=True)'.&quot;
        )

    if (
        use_pyarrow
        and dtypes is None
        and n_rows is None
        and n_threads is None
        and encoding == &quot;utf8&quot;
        and not low_memory
        and null_values is None
        and parse_dates
    ):
        include_columns = None

        if columns:
            if not has_header:
                # Convert 'column_1', 'column_2', ... column names to 'f0', 'f1', ... column names for pyarrow,
                # if CSV file does not contain a header.
                include_columns = [f&quot;f{int(column[7:]) - 1}&quot; for column in columns]
            else:
                include_columns = columns

        if not columns and projection:
            # Convert column indices from projection to 'f0', 'f1', ... column names for pyarrow.
            include_columns = [f&quot;f{column_idx}&quot; for column_idx in projection]

        with _prepare_file_arg(file, **storage_options) as data:
            tbl = pa.csv.read_csv(
                data,
                pa.csv.ReadOptions(
                    skip_rows=skip_rows, autogenerate_column_names=not has_header
                ),
                pa.csv.ParseOptions(delimiter=sep),
                pa.csv.ConvertOptions(
                    column_types=None,
                    include_columns=include_columns,
                    include_missing_columns=ignore_errors,
                ),
            )

        if not has_header:
            # Rename 'f0', 'f1', ... columns names autogenated by pyarrow to 'column_1', 'column_2', ...
            tbl = tbl.rename_columns(
                [f&quot;column_{int(column[1:]) + 1}&quot; for column in tbl.column_names]
            )

        df = from_arrow(tbl, rechunk)
        if new_columns:
            return update_columns(df, new_columns)  # type: ignore
        return df  # type: ignore

    if new_columns and dtypes and isinstance(dtypes, dict):
        current_columns = None

        # As new column names are not available yet while parsing the CSV file, rename column names in
        # dtypes to old names (if possible) so they can be used during CSV parsing.
        if columns:
            if len(columns) &lt; len(new_columns):
                raise ValueError(
                    &quot;More new colum names are specified than there are selected columns.&quot;
                )

            # Get column names of requested columns.
            current_columns = columns[0 : len(new_columns)]
        elif not has_header:
            # When there are no header, column names are autogenerated (and known).

            if projection:
                if columns and len(columns) &lt; len(new_columns):
                    raise ValueError(
                        &quot;More new colum names are specified than there are selected columns.&quot;
                    )
                # Convert column indices from projection to 'column_1', 'column_2', ... column names.
                current_columns = [
                    f&quot;column_{column_idx + 1}&quot; for column_idx in projection
                ]
            else:
                # Generate autogenerated 'column_1', 'column_2', ... column names for new column names.
                current_columns = [
                    f&quot;column_{column_idx}&quot;
                    for column_idx in range(1, len(new_columns) + 1)
                ]
        else:
            # When a header is present, column names are not known yet.

            if len(dtypes) &lt;= len(new_columns):
                # If dtypes dictionary contains less or same amount of values than new column names
                # a list of dtypes can be created if all listed column names in dtypes dictionary
                # appear in the first consecutive new column names.
                dtype_list = [
                    dtypes[new_column_name]
                    for new_column_name in new_columns[0 : len(dtypes)]
                    if new_column_name in dtypes
                ]

                if len(dtype_list) == len(dtypes):
                    dtypes = dtype_list

        if current_columns and isinstance(dtypes, dict):
            new_to_current = {
                new_column: current_column
                for new_column, current_column in zip(new_columns, current_columns)
            }
            # Change new column names to current column names in dtype.
            dtypes = {
                new_to_current.get(column_name, column_name): column_dtype
                for column_name, column_dtype in dtypes.items()
            }

    with _prepare_file_arg(file, **storage_options) as data:
        df = DataFrame._read_csv(
            file=data,
            has_header=has_header,
            columns=columns if columns else projection,
            sep=sep,
            comment_char=comment_char,
            quote_char=quote_char,
            skip_rows=skip_rows,
            dtypes=dtypes,
            null_values=null_values,
            ignore_errors=ignore_errors,
            parse_dates=parse_dates,
            n_threads=n_threads,
            infer_schema_length=infer_schema_length,
            batch_size=batch_size,
            n_rows=n_rows,
            encoding=encoding,
            low_memory=low_memory,
            rechunk=rechunk,
            offset_schema_inference=offset_schema_inference,
        )

    if new_columns:
        return update_columns(df, new_columns)
    return df
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioscan_csv"><a class="header" href="#polarsioscan_csv"><code>polars.io.scan_csv</code></a></h3>
<pre><code class="language-python">scan_csv(
    file: Union[str, Path], 
    has_header: bool, 
    sep: str, 
    comment_char: Optionalstr, 
    quote_char: Optionalstr, 
    skip_rows: int, 
    dtypes: OptionalDict[str, TypeDataType], 
    null_values: OptionalUnion[str, List[str], Dict[str, str]], 
    ignore_errors: bool, 
    cache: bool, 
    with_column_names: OptionalCallable[[List[str]], List[str]], 
    infer_schema_length: Optionalint, 
    n_rows: Optionalint, 
    low_memory: bool, 
    rechunk: bool, 
    offset_schema_inference: int, 
    **kwargs,
) -&gt; LazyFrame:
</code></pre>
<p>Lazily read from a CSV file or multiple files via glob patterns.</p>
<p>This allows the query optimizer to push down predicates and
projections to the scan level, thereby potentially reducing
memory overhead.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>file</code>]: Path to a file.</li>
<li>[<code>has_header</code>]: Indicate if the first row of dataset is a header or not.
If set to False, column names will be autogenrated in the
following format: <code>column_x</code>, with <code>x</code> being an
enumeration over every column in the dataset starting at 1.</li>
<li>[<code>sep</code>]: Character to use as delimiter in the file.</li>
<li>[<code>comment_char</code>]: Character that indicates the start of a comment line, for
instance <code>#</code>.</li>
<li>[<code>quote_char</code>]: Single byte character used for csv quoting, default = ''.
Set to None to turn off special handling and escaping of quotes.</li>
<li>[<code>skip_rows</code>]: Start reading after <code>skip_rows</code> lines.</li>
<li>[<code>dtypes</code>]: Overwrite dtypes during inference.</li>
<li>[<code>null_values</code>]: Values to interpret as null values. You can provide a:
- <code>str</code>: All values equal to this string will be null.
- <code>List[str]</code>: A null value per column.
- <code>Dict[str, str]</code>: A dictionary that maps column name to a
null value string.</li>
<li>[<code>ignore_errors</code>]: Try to keep reading lines if some lines yield errors.
First try <code>infer_schema_length=0</code> to read all columns as
<code>pl.Utf8</code> to check which values might cause an issue.</li>
<li>[<code>cache</code>]: Cache the result after reading.</li>
<li>[<code>with_column_names</code>]: Apply a function over the column names.
This can be used to update a schema just in time, thus before
scanning.</li>
<li>[<code>infer_schema_length</code>]: Maximum number of lines to read to infer schema.
If set to 0, all columns will be read as <code>pl.Utf8</code>.
If set to <code>None</code>, a full table scan will be done (slow).</li>
<li>[<code>n_rows</code>]: Stop reading from CSV file after reading <code>n_rows</code>.</li>
<li>[<code>low_memory</code>]: Reduce memory usage in expense of performance.</li>
<li>[<code>rechunk</code>]: Reallocate to contiguous memory when all chunks/ files are parsed.</li>
<li>[<code>offset_schema_inference</code>]: Start schema parsing of the header at this offset</li>
</ul>
<p><strong>Examples</strong></p>
<blockquote>
<blockquote>
<blockquote>
<p>(
...     pl.scan_csv(&quot;my_long_file.csv&quot;)  # lazy, doesn't do a thing
...     .select(
...         [&quot;a&quot;, &quot;c&quot;]
...     )  # select only 2 columns (other columns will not be read)
...     .filter(
...         pl.col(&quot;a&quot;) &gt; 10
...     )  # the filter is pushed down the the scan, so less data read in memory
...     .fetch(100)  # pushed a limit of 100 rows to the scan level
... )  # doctest: +SKIP</p>
</blockquote>
</blockquote>
</blockquote>
<p>We can use <code>with_column_names</code> to modify the header before scanning:</p>
<blockquote>
<blockquote>
<blockquote>
<p>df = pl.DataFrame(
...     {&quot;BrEeZaH&quot;: [1, 2, 3, 4], &quot;LaNgUaGe&quot;: [&quot;is&quot;, &quot;terrible&quot;, &quot;to&quot;, &quot;read&quot;]}
... )
df.to_csv(&quot;mydf.csv&quot;)
pl.scan_csv(
...     &quot;mydf.csv&quot;, with_column_names=lambda cols: [col.lower() for col in cols]
... ).fetch()
shape: (4, 2)
┌─────────┬──────────┐
│ breezah ┆ language │
│ ---     ┆ ---      │
│ i64     ┆ str      │
╞═════════╪══════════╡
│ 1       ┆ is       │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 2       ┆ terrible │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 3       ┆ to       │
├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
│ 4       ┆ read     │
└─────────┴──────────┘</p>
</blockquote>
</blockquote>
</blockquote>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def scan_csv(
    file: Union[str, Path],
    has_header: bool = True,
    sep: str = &quot;,&quot;,
    comment_char: Optional[str] = None,
    quote_char: Optional[str] = r'&quot;',
    skip_rows: int = 0,
    dtypes: Optional[Dict[str, Type[DataType]]] = None,
    null_values: Optional[Union[str, List[str], Dict[str, str]]] = None,
    ignore_errors: bool = False,
    cache: bool = True,
    with_column_names: Optional[Callable[[List[str]], List[str]]] = None,
    infer_schema_length: Optional[int] = 100,
    n_rows: Optional[int] = None,
    low_memory: bool = False,
    rechunk: bool = True,
    offset_schema_inference: int = 0,
    **kwargs: Any,
) -&gt; LazyFrame:
    &quot;&quot;&quot;
    Lazily read from a CSV file or multiple files via glob patterns.

    This allows the query optimizer to push down predicates and
    projections to the scan level, thereby potentially reducing
    memory overhead.

    Parameters
    ----------
    file
        Path to a file.
    has_header
        Indicate if the first row of dataset is a header or not.
        If set to False, column names will be autogenrated in the
        following format: ``column_x``, with ``x`` being an
        enumeration over every column in the dataset starting at 1.
    sep
        Character to use as delimiter in the file.
    comment_char
        Character that indicates the start of a comment line, for
        instance ``#``.
    quote_char
        Single byte character used for csv quoting, default = ''.
        Set to None to turn off special handling and escaping of quotes.
    skip_rows
        Start reading after ``skip_rows`` lines.
    dtypes
        Overwrite dtypes during inference.
    null_values
        Values to interpret as null values. You can provide a:
          - ``str``: All values equal to this string will be null.
          - ``List[str]``: A null value per column.
          - ``Dict[str, str]``: A dictionary that maps column name to a
                                null value string.
    ignore_errors
        Try to keep reading lines if some lines yield errors.
        First try ``infer_schema_length=0`` to read all columns as
        ``pl.Utf8`` to check which values might cause an issue.
    cache
        Cache the result after reading.
    with_column_names
        Apply a function over the column names.
        This can be used to update a schema just in time, thus before
        scanning.
    infer_schema_length
        Maximum number of lines to read to infer schema.
        If set to 0, all columns will be read as ``pl.Utf8``.
        If set to ``None``, a full table scan will be done (slow).
    n_rows
        Stop reading from CSV file after reading ``n_rows``.
    low_memory
        Reduce memory usage in expense of performance.
    rechunk
        Reallocate to contiguous memory when all chunks/ files are parsed.
    offset_schema_inference
        Start schema parsing of the header at this offset

    Examples
    --------
    &gt;&gt;&gt; (
    ...     pl.scan_csv(&quot;my_long_file.csv&quot;)  # lazy, doesn't do a thing
    ...     .select(
    ...         [&quot;a&quot;, &quot;c&quot;]
    ...     )  # select only 2 columns (other columns will not be read)
    ...     .filter(
    ...         pl.col(&quot;a&quot;) &gt; 10
    ...     )  # the filter is pushed down the the scan, so less data read in memory
    ...     .fetch(100)  # pushed a limit of 100 rows to the scan level
    ... )  # doctest: +SKIP

    We can use `with_column_names` to modify the header before scanning:

    &gt;&gt;&gt; df = pl.DataFrame(
    ...     {&quot;BrEeZaH&quot;: [1, 2, 3, 4], &quot;LaNgUaGe&quot;: [&quot;is&quot;, &quot;terrible&quot;, &quot;to&quot;, &quot;read&quot;]}
    ... )
    &gt;&gt;&gt; df.to_csv(&quot;mydf.csv&quot;)
    &gt;&gt;&gt; pl.scan_csv(
    ...     &quot;mydf.csv&quot;, with_column_names=lambda cols: [col.lower() for col in cols]
    ... ).fetch()
    shape: (4, 2)
    ┌─────────┬──────────┐
    │ breezah ┆ language │
    │ ---     ┆ ---      │
    │ i64     ┆ str      │
    ╞═════════╪══════════╡
    │ 1       ┆ is       │
    ├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
    │ 2       ┆ terrible │
    ├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
    │ 3       ┆ to       │
    ├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌┤
    │ 4       ┆ read     │
    └─────────┴──────────┘


    &quot;&quot;&quot;

    # Map legacy arguments to current ones and remove them from kwargs.
    has_header = kwargs.pop(&quot;has_headers&quot;, has_header)
    dtypes = kwargs.pop(&quot;dtype&quot;, dtypes)
    n_rows = kwargs.pop(&quot;stop_after_n_rows&quot;, n_rows)

    if isinstance(file, Path):
        file = str(file)

    return LazyFrame.scan_csv(
        file=file,
        has_header=has_header,
        sep=sep,
        comment_char=comment_char,
        quote_char=quote_char,
        skip_rows=skip_rows,
        dtypes=dtypes,
        null_values=null_values,
        ignore_errors=ignore_errors,
        cache=cache,
        with_column_names=with_column_names,
        infer_schema_length=infer_schema_length,
        n_rows=n_rows,
        low_memory=low_memory,
        rechunk=rechunk,
        offset_schema_inference=offset_schema_inference,
    )
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioscan_ipc"><a class="header" href="#polarsioscan_ipc"><code>polars.io.scan_ipc</code></a></h3>
<pre><code class="language-python">scan_ipc(
    file: Union[str, Path], 
    n_rows: Optionalint, 
    cache: bool, 
    rechunk: bool, 
    **kwargs,
) -&gt; LazyFrame:
</code></pre>
<p>Lazily read from an Arrow IPC (Feather v2) file or multiple files via glob patterns.</p>
<p>This allows the query optimizer to push down predicates and projections to the scan level,
thereby potentially reducing memory overhead.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>file</code>]: Path to a IPC file.</li>
<li>[<code>n_rows</code>]: Stop reading from IPC file after reading <code>n_rows</code>.</li>
<li>[<code>cache</code>]: Cache the result after reading.</li>
<li>[<code>rechunk</code>]: Reallocate to contiguous memory when all chunks/ files are parsed.</li>
</ul>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def scan_ipc(
    file: Union[str, Path],
    n_rows: Optional[int] = None,
    cache: bool = True,
    rechunk: bool = True,
    **kwargs: Any,
) -&gt; LazyFrame:
    &quot;&quot;&quot;
    Lazily read from an Arrow IPC (Feather v2) file or multiple files via glob patterns.

    This allows the query optimizer to push down predicates and projections to the scan level,
    thereby potentially reducing memory overhead.

    Parameters
    ----------
    file
        Path to a IPC file.
    n_rows
        Stop reading from IPC file after reading ``n_rows``.
    cache
        Cache the result after reading.
    rechunk
        Reallocate to contiguous memory when all chunks/ files are parsed.
    &quot;&quot;&quot;

    # Map legacy arguments to current ones and remove them from kwargs.
    n_rows = kwargs.pop(&quot;stop_after_n_rows&quot;, n_rows)

    if isinstance(file, Path):
        file = str(file)

    return LazyFrame.scan_ipc(file=file, n_rows=n_rows, cache=cache, rechunk=rechunk)
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioscan_parquet"><a class="header" href="#polarsioscan_parquet"><code>polars.io.scan_parquet</code></a></h3>
<pre><code class="language-python">scan_parquet(
    file: Union[str, Path], 
    n_rows: Optionalint, 
    cache: bool, 
    parallel: bool, 
    rechunk: bool, 
    **kwargs,
) -&gt; LazyFrame:
</code></pre>
<p>Lazily read from a parquet file or multiple files via glob patterns.</p>
<p>This allows the query optimizer to push down predicates and projections to the scan level,
thereby potentially reducing memory overhead.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>file</code>]: Path to a file.</li>
<li>[<code>n_rows</code>]: Stop reading from parquet file after reading <code>n_rows</code>.</li>
<li>[<code>cache</code>]: Cache the result after reading.</li>
<li>[<code>parallel</code>]: Read the parquet file in parallel. The single threaded reader consumes less memory.</li>
<li>[<code>rechunk</code>]: In case of reading multiple files via a glob pattern rechunk the final DataFrame into contiguous memory chunks.</li>
</ul>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def scan_parquet(
    file: Union[str, Path],
    n_rows: Optional[int] = None,
    cache: bool = True,
    parallel: bool = True,
    rechunk: bool = True,
    **kwargs: Any,
) -&gt; LazyFrame:
    &quot;&quot;&quot;
    Lazily read from a parquet file or multiple files via glob patterns.

    This allows the query optimizer to push down predicates and projections to the scan level,
    thereby potentially reducing memory overhead.

    Parameters
    ----------
    file
        Path to a file.
    n_rows
        Stop reading from parquet file after reading ``n_rows``.
    cache
        Cache the result after reading.
    parallel
        Read the parquet file in parallel. The single threaded reader consumes less memory.
    rechunk
        In case of reading multiple files via a glob pattern rechunk the final DataFrame into contiguous memory chunks.
    &quot;&quot;&quot;

    # Map legacy arguments to current ones and remove them from kwargs.
    n_rows = kwargs.pop(&quot;stop_after_n_rows&quot;, n_rows)

    if isinstance(file, Path):
        file = str(file)

    return LazyFrame.scan_parquet(
        file=file, n_rows=n_rows, cache=cache, parallel=parallel, rechunk=rechunk
    )
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioread_ipc_schema"><a class="header" href="#polarsioread_ipc_schema"><code>polars.io.read_ipc_schema</code></a></h3>
<pre><code class="language-python">read_ipc_schema(
    file: Union[str, BinaryIO, Path, bytes],
) -&gt; Dict[str, TypeDataType]:
</code></pre>
<p>Get a schema of the IPC file without reading data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>file</code>]: Path to a file or a file like object.</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary mapping column names to datatypes</p>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def read_ipc_schema(
    file: Union[str, BinaryIO, Path, bytes]
) -&gt; Dict[str, Type[DataType]]:
    &quot;&quot;&quot;
    Get a schema of the IPC file without reading data.

    Parameters
    ----------
    file
        Path to a file or a file like object.

    Returns
    -------
    Dictionary mapping column names to datatypes
    &quot;&quot;&quot;
    return _ipc_schema(file)
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioread_ipc"><a class="header" href="#polarsioread_ipc"><code>polars.io.read_ipc</code></a></h3>
<pre><code class="language-python">read_ipc(
    file: Union[str, BinaryIO, BytesIO, Path, bytes], 
    columns: OptionalUnion[List[int], List[str]], 
    n_rows: Optionalint, 
    use_pyarrow: bool, 
    memory_map: bool, 
    storage_options: OptionalDict, 
    **kwargs,
) -&gt; DataFrame:
</code></pre>
<p>Read into a DataFrame from Arrow IPC (Feather v2) file.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>file</code>]: Path to a file or a file like object.
If <code>fsspec</code> is installed, it will be used to open remote files.</li>
<li>[<code>columns</code>]: Columns to select. Accepts a list of column indices (starting at zero) or a list of column names.</li>
<li>[<code>n_rows</code>]: Stop reading from IPC file after reading <code>n_rows</code>.
Only valid when <code>use_pyarrow=False</code>.</li>
<li>[<code>use_pyarrow</code>]: Use pyarrow or the native rust reader.</li>
<li>[<code>memory_map</code>]: Memory map underlying file. This will likely increase performance.
Only used when <code>use_pyarrow=True</code>.</li>
<li>[<code>storage_options</code>]: Extra options that make sense for <code>fsspec.open()</code> or a particular storage connection, e.g. host, port, username, password, etc.</li>
</ul>
<p><strong>Returns</strong></p>
<p>DataFrame</p>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def read_ipc(
    file: Union[str, BinaryIO, BytesIO, Path, bytes],
    columns: Optional[Union[List[int], List[str]]] = None,
    n_rows: Optional[int] = None,
    use_pyarrow: bool = _PYARROW_AVAILABLE,
    memory_map: bool = True,
    storage_options: Optional[Dict] = None,
    **kwargs: Any,
) -&gt; DataFrame:
    &quot;&quot;&quot;
    Read into a DataFrame from Arrow IPC (Feather v2) file.

    Parameters
    ----------
    file
        Path to a file or a file like object.
        If ``fsspec`` is installed, it will be used to open remote files.
    columns
        Columns to select. Accepts a list of column indices (starting at zero) or a list of column names.
    n_rows
        Stop reading from IPC file after reading ``n_rows``.
        Only valid when `use_pyarrow=False`.
    use_pyarrow
        Use pyarrow or the native rust reader.
    memory_map
        Memory map underlying file. This will likely increase performance.
        Only used when ``use_pyarrow=True``.
    storage_options
        Extra options that make sense for ``fsspec.open()`` or a particular storage connection, e.g. host, port, username, password, etc.

    Returns
    -------
    DataFrame
    &quot;&quot;&quot;

    # Map legacy arguments to current ones and remove them from kwargs.
    n_rows = kwargs.pop(&quot;stop_after_n_rows&quot;, n_rows)

    if columns is None:
        columns = kwargs.pop(&quot;projection&quot;, None)

    if use_pyarrow:
        if n_rows:
            raise ValueError(&quot;``n_rows`` cannot be used with ``use_pyarrow=True``.&quot;)

    storage_options = storage_options or {}
    with _prepare_file_arg(file, **storage_options) as data:
        if use_pyarrow:
            if not _PYARROW_AVAILABLE:
                raise ImportError(
                    &quot;'pyarrow' is required when using 'read_ipc(..., use_pyarrow=True)'.&quot;
                )

            tbl = pa.feather.read_table(data, memory_map=memory_map, columns=columns)
            return DataFrame._from_arrow(tbl)

        return DataFrame._read_ipc(
            data,
            columns=columns,
            n_rows=n_rows,
        )
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioread_parquet"><a class="header" href="#polarsioread_parquet"><code>polars.io.read_parquet</code></a></h3>
<pre><code class="language-python">read_parquet(
    source: Union[str, List[str], Path, BinaryIO, BytesIO, bytes], 
    columns: OptionalUnion[List[int], List[str]], 
    n_rows: Optionalint, 
    use_pyarrow: bool, 
    memory_map: bool, 
    storage_options: OptionalDict, 
    parallel: bool, 
    **kwargs,
) -&gt; DataFrame:
</code></pre>
<p>Read into a DataFrame from a parquet file.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>source</code>]: Path to a file, list of files, or a file like object. If the path is a directory, that directory will be used
as partition aware scan.
If <code>fsspec</code> is installed, it will be used to open remote files.</li>
<li>[<code>columns</code>]: Columns to select. Accepts a list of column indices (starting at zero) or a list of column names.</li>
<li>[<code>n_rows</code>]: Stop reading from parquet file after reading <code>n_rows</code>.
Only valid when <code>use_pyarrow=False</code>.</li>
<li>[<code>use_pyarrow</code>]: Use pyarrow instead of the rust native parquet reader. The pyarrow reader is more stable.</li>
<li>[<code>memory_map</code>]: Memory map underlying file. This will likely increase performance.
Only used when <code>use_pyarrow=True</code>.</li>
<li>[<code>storage_options</code>]: Extra options that make sense for <code>fsspec.open()</code> or a particular storage connection, e.g. host, port, username, password, etc.</li>
<li>[<code>parallel</code>]: Read the parquet file in parallel. The single threaded reader consumes less memory.
**kwargs
kwargs for <a href="https:/arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html">pyarrow.parquet.read_table</a></li>
</ul>
<p><strong>Returns</strong></p>
<p>DataFrame</p>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def read_parquet(
    source: Union[str, List[str], Path, BinaryIO, BytesIO, bytes],
    columns: Optional[Union[List[int], List[str]]] = None,
    n_rows: Optional[int] = None,
    use_pyarrow: bool = False,
    memory_map: bool = True,
    storage_options: Optional[Dict] = None,
    parallel: bool = True,
    **kwargs: Any,
) -&gt; DataFrame:
    &quot;&quot;&quot;
    Read into a DataFrame from a parquet file.

    Parameters
    ----------
    source
        Path to a file, list of files, or a file like object. If the path is a directory, that directory will be used
        as partition aware scan.
        If ``fsspec`` is installed, it will be used to open remote files.
    columns
        Columns to select. Accepts a list of column indices (starting at zero) or a list of column names.
    n_rows
        Stop reading from parquet file after reading ``n_rows``.
        Only valid when `use_pyarrow=False`.
    use_pyarrow
        Use pyarrow instead of the rust native parquet reader. The pyarrow reader is more stable.
    memory_map
        Memory map underlying file. This will likely increase performance.
        Only used when ``use_pyarrow=True``.
    storage_options
        Extra options that make sense for ``fsspec.open()`` or a particular storage connection, e.g. host, port, username, password, etc.
    parallel
        Read the parquet file in parallel. The single threaded reader consumes less memory.
    **kwargs
        kwargs for [pyarrow.parquet.read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)

    Returns
    -------
    DataFrame
    &quot;&quot;&quot;

    # Map legacy arguments to current ones and remove them from kwargs.
    n_rows = kwargs.pop(&quot;stop_after_n_rows&quot;, n_rows)

    if columns is None:
        columns = kwargs.pop(&quot;projection&quot;, None)

    if use_pyarrow:
        if n_rows:
            raise ValueError(&quot;``n_rows`` cannot be used with ``use_pyarrow=True``.&quot;)

    storage_options = storage_options or {}
    with _prepare_file_arg(source, **storage_options) as source_prep:
        if use_pyarrow:
            if not _PYARROW_AVAILABLE:
                raise ImportError(
                    &quot;'pyarrow' is required when using 'read_parquet(..., use_pyarrow=True)'.&quot;
                )

            return from_arrow(  # type: ignore[return-value]
                pa.parquet.read_table(
                    source_prep,
                    memory_map=memory_map,
                    columns=columns,
                    **kwargs,
                )
            )

        return DataFrame._read_parquet(
            source_prep, columns=columns, n_rows=n_rows, parallel=parallel
        )
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioread_json"><a class="header" href="#polarsioread_json"><code>polars.io.read_json</code></a></h3>
<pre><code class="language-python">read_json(source: Union[str, BytesIO]) -&gt; DataFrame:
</code></pre>
<p>Read into a DataFrame from JSON format.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>source</code>]: Path to a file or a file like object.</li>
</ul>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def read_json(source: Union[str, BytesIO]) -&gt; DataFrame:
    &quot;&quot;&quot;
    Read into a DataFrame from JSON format.

    Parameters
    ----------
    source
        Path to a file or a file like object.
    &quot;&quot;&quot;
    return DataFrame._read_json(source)
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
<div class='function-wrap'></raw></p>
<h3 id="polarsioread_sql"><a class="header" href="#polarsioread_sql"><code>polars.io.read_sql</code></a></h3>
<pre><code class="language-python">read_sql(
    sql: Union[List[str], str], 
    connection_uri: str, 
    partition_on: Optionalstr, 
    partition_range: OptionalTuple[int, int], 
    partition_num: Optionalint, 
    protocol: Optionalstr,
) -&gt; DataFrame:
</code></pre>
<p>Read a SQL query into a DataFrame.
Make sure to install connectorx&gt;=0.2</p>
<p><strong>Sources</strong></p>
<p>Supports reading a sql query from the following data sources:</p>
<ul>
<li>Postgres</li>
<li>Mysql</li>
<li>Sqlite</li>
<li>Redshift (through postgres protocol)</li>
<li>Clickhouse (through mysql protocol)</li>
</ul>
<h2 id="source-not-supported"><a class="header" href="#source-not-supported">Source not supported?</a></h2>
<p>If a database source is not supported, pandas can be used to load the query:</p>
<blockquote>
<blockquote>
<blockquote>
<p>import pandas as pd
df = pl.from_pandas(pd.read_sql(sql, engine))  # doctest: +SKIP</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters</strong></p>
<ul>
<li>[<code>sql</code>]: raw sql query.</li>
<li>[<code>connection_uri</code>]: connectorx connection uri:
- &quot;postgresql:/username:password@server:port/database&quot;</li>
<li>[<code>partition_on</code>]: the column on which to partition the result.</li>
<li>[<code>partition_range</code>]: the value range of the partition column.</li>
<li>[<code>partition_num</code>]: how many partitions to generate.</li>
<li>[<code>protocol</code>]: backend-specific transfer protocol directive; see connectorx documentation for details.</li>
</ul>
<p><strong>Examples</strong></p>
<h2 id="single-threaded"><a class="header" href="#single-threaded">Single threaded</a></h2>
<p>Read a DataFrame from a SQL query using a single thread:</p>
<blockquote>
<blockquote>
<blockquote>
<p>uri = &quot;postgresql:/username:password@server:port/database&quot;
query = &quot;SELECT * FROM lineitem&quot;
pl.read_sql(query, uri)  # doctest: +SKIP</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="using-10-threads"><a class="header" href="#using-10-threads">Using 10 threads</a></h2>
<p>Read a DataFrame in parallel using 10 threads by automatically partitioning the provided SQL on the partition column:</p>
<blockquote>
<blockquote>
<blockquote>
<p>uri = &quot;postgresql:/username:password@server:port/database&quot;
query = &quot;SELECT * FROM lineitem&quot;
pl.read_sql(
...     query, uri, partition_on=&quot;partition_col&quot;, partition_num=10
... )  # doctest: +SKIP</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="using"><a class="header" href="#using">Using</a></h2>
<p>Read a DataFrame in parallel using 2 threads by explicitly providing two SQL queries:</p>
<blockquote>
<blockquote>
<blockquote>
<p>uri = &quot;postgresql:/username:password@server:port/database&quot;
queries = [
...     &quot;SELECT * FROM lineitem WHERE partition_col &lt;= 10&quot;,
...     &quot;SELECT * FROM lineitem WHERE partition_col &gt; 10&quot;,
... ]
pl.read_sql(uri, queries)  # doctest: +SKIP</p>
</blockquote>
</blockquote>
</blockquote>
<p>
<details>
  <summary style="text-align:right">source</summary>
</raw></p>
<pre><code class="language-python">def read_sql(
    sql: Union[List[str], str],
    connection_uri: str,
    partition_on: Optional[str] = None,
    partition_range: Optional[Tuple[int, int]] = None,
    partition_num: Optional[int] = None,
    protocol: Optional[str] = None,
) -&gt; DataFrame:
    &quot;&quot;&quot;
    Read a SQL query into a DataFrame.
    Make sure to install connectorx&gt;=0.2

    # Sources
    Supports reading a sql query from the following data sources:

    * Postgres
    * Mysql
    * Sqlite
    * Redshift (through postgres protocol)
    * Clickhouse (through mysql protocol)

    ## Source not supported?
    If a database source is not supported, pandas can be used to load the query:

    &gt;&gt;&gt; import pandas as pd
    &gt;&gt;&gt; df = pl.from_pandas(pd.read_sql(sql, engine))  # doctest: +SKIP

    Parameters
    ----------
    sql
        raw sql query.
    connection_uri
        connectorx connection uri:
            - &quot;postgresql://username:password@server:port/database&quot;
    partition_on
      the column on which to partition the result.
    partition_range
      the value range of the partition column.
    partition_num
      how many partitions to generate.
    protocol
      backend-specific transfer protocol directive; see connectorx documentation for details.

    Examples
    --------

    ## Single threaded
    Read a DataFrame from a SQL query using a single thread:

    &gt;&gt;&gt; uri = &quot;postgresql://username:password@server:port/database&quot;
    &gt;&gt;&gt; query = &quot;SELECT * FROM lineitem&quot;
    &gt;&gt;&gt; pl.read_sql(query, uri)  # doctest: +SKIP

    ## Using 10 threads
    Read a DataFrame in parallel using 10 threads by automatically partitioning the provided SQL on the partition column:

    &gt;&gt;&gt; uri = &quot;postgresql://username:password@server:port/database&quot;
    &gt;&gt;&gt; query = &quot;SELECT * FROM lineitem&quot;
    &gt;&gt;&gt; pl.read_sql(
    ...     query, uri, partition_on=&quot;partition_col&quot;, partition_num=10
    ... )  # doctest: +SKIP

    ## Using
    Read a DataFrame in parallel using 2 threads by explicitly providing two SQL queries:

    &gt;&gt;&gt; uri = &quot;postgresql://username:password@server:port/database&quot;
    &gt;&gt;&gt; queries = [
    ...     &quot;SELECT * FROM lineitem WHERE partition_col &lt;= 10&quot;,
    ...     &quot;SELECT * FROM lineitem WHERE partition_col &gt; 10&quot;,
    ... ]
    &gt;&gt;&gt; pl.read_sql(uri, queries)  # doctest: +SKIP

    &quot;&quot;&quot;
    if _WITH_CX:
        tbl = cx.read_sql(
            conn=connection_uri,
            query=sql,
            return_type=&quot;arrow&quot;,
            partition_on=partition_on,
            partition_range=partition_range,
            partition_num=partition_num,
            protocol=protocol,
        )
        return from_arrow(tbl)  # type: ignore[return-value]
    else:
        raise ImportError(
            &quot;connectorx is not installed.&quot; &quot;Please run pip install connectorx&gt;=0.2.2&quot;
        )
</code></pre>
<p>
</details>
</raw></p>
<p></div></raw>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../polars/internals/whenthen/When.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../polars/string_cache.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../polars/internals/whenthen/When.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="../polars/string_cache.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
        <script type="text/javascript" src="../theme/js/index.js"></script>
        

        

    </body>
</html>
